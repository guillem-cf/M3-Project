{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 1\n",
    "- Guillem\n",
    "- Anna\n",
    "- Johnny"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "● Test different amounts of local features. What performs best?  \n",
    "● Use dense SIFT instead of detected keypoints. Conclusions?  \n",
    "● Test different amounts of codebook sizes k. What performs best?  \n",
    "● Test different values of k for the k-nn classifier. What performs best?  \n",
    "● Test other distances in k-nn classifier. Does that make a difference? Why?  \n",
    "● Play with reducing dimensionality. Conclusions?  \n",
    "● Cross-validate everything (topic covered on Wednesday)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import plot_contour, plot_edf, plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, plot_param_importances, plot_slice\n",
    "import os\n",
    "from optuna.samplers import TPESampler\n",
    "import concurrent.futures\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_filenames = pickle.load(\n",
    "    open('MIT_split/train_images_filenames_unix.dat', 'rb'))\n",
    "test_images_filenames = pickle.load(\n",
    "    open('MIT_split/test_images_filenames_unix.dat', 'rb'))\n",
    "# train_images_filenames = ['..' + n[15:] for n in train_images_filenames] original\n",
    "# test_images_filenames  = ['..' + n[15:] for n in test_images_filenames]  original\n",
    "train_images_filenames = [n[16:] for n in train_images_filenames]\n",
    "test_images_filenames = [n[16:] for n in test_images_filenames]\n",
    "train_labels = pickle.load(open('MIT_split/train_labels_unix.dat', 'rb'))\n",
    "test_labels = pickle.load(open('MIT_split/test_labels_unix.dat', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_filenames[12]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distribution Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes, counts = np.unique(train_labels, return_counts=True)\n",
    "total_count = sum(counts)\n",
    "train_class_proportions = counts / total_count\n",
    "\n",
    "# Calculate the class proportions for the test set\n",
    "unique_classes, counts = np.unique(test_labels, return_counts=True)\n",
    "total_count = sum(counts)\n",
    "test_class_proportions = counts / total_count\n",
    "\n",
    "# Print the class proportions for the train and test sets\n",
    "print(\"Train set class proportions:\", train_class_proportions)\n",
    "print(\"Test set class proportions:\", test_class_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(train_labels + test_labels)\n",
    "train_count = [np.sum(np.array(train_labels) == lab) for lab in unique_labels]\n",
    "test_count = [np.sum(np.array(test_labels) == lab) for lab in unique_labels]\n",
    "\n",
    "\n",
    "# distribution of the training and test set\n",
    "def plot_distribution(train_count, test_count, unique_labels):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.title(\"Distribution of the training and test set\")\n",
    "    plt.bar(unique_labels, train_count, label=\"Training Set\")\n",
    "    plt.bar(unique_labels, test_count, label=\"Test Set\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(train_count, test_count, unique_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: EXPLANATION\n",
    "To check if a dataset is unbalanced, we can calculate the proportion of each class in the dataset and compare the proportions. If the proportions of the classes are significantly different, then the dataset is likely to be unbalanced.\n",
    "In this case, the dataset is balanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAZE:\n",
    "    def __init__(self, threshold=0.001):\n",
    "        self.extractor = cv2.KAZE_create(threshold=threshold)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        keypoints, descriptors = self.extractor.detectAndCompute(image, None)\n",
    "        return descriptors\n",
    "\n",
    "\n",
    "class AKAZE:\n",
    "    def __init__(self, threshold=0.001):\n",
    "        self.extractor = cv2.AKAZE_create(threshold=threshold)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        keypoints, descriptors = self.extractor.detectAndCompute(image, None)\n",
    "        return descriptors\n",
    "\n",
    "\n",
    "class SIFT:\n",
    "    def __init__(self, n_features=100):\n",
    "        self.extractor = cv2.xfeatures2d.SIFT_create(nfeatures=n_features)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        keypoints, descriptors = self.extractor.detectAndCompute(image, None)\n",
    "        return descriptors\n",
    "\n",
    "\n",
    "class DenseSIFT:\n",
    "    def __init__(self, step_size=10, patch_size=10):\n",
    "        self.extractor = cv2.xfeatures2d.SIFT_create()\n",
    "        self.step_size = step_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        \n",
    "        init_step_size_x = max(image.shape[1] // self.step_size, 16)\n",
    "        init_step_size_y = max(image.shape[0] // self.step_size, 16)\n",
    "\n",
    "        descriptors = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for i in range(1, self.patch_size+1):\n",
    "                current_step_x = init_step_size_x * i\n",
    "                current_step_y = init_step_size_y * i\n",
    "                avg_size = (current_step_x + current_step_y) // 2\n",
    "                for y in range(0, image.shape[0], current_step_y):\n",
    "                    for x in range(0, image.shape[1], current_step_x):\n",
    "                        sub_image = image[y:y+current_step_y, x:x+current_step_x]\n",
    "                        keypoint = cv2.KeyPoint(x, y, avg_size)\n",
    "                        futures.append(executor.submit(SIFT().detector.compute, sub_image, [keypoint]))\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                _, sub_descriptors = future.result()\n",
    "                descriptors.append(sub_descriptors)\n",
    "\n",
    "        return np.concatenate(descriptors)\n",
    "\n",
    "\n",
    "class ORB:\n",
    "    def __init__(self, n_features=100):\n",
    "        self.extractor = cv2.ORB_create(nfeatures=n_features)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        keypoints, descriptors = self.extractor.detectAndCompute(image, None)\n",
    "        return descriptors\n",
    "\n",
    "\n",
    "class BRISK:\n",
    "    def __init__(self, n_features=100):\n",
    "        self.extractor = cv2.BRISK_create(nfeatures=n_features)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        keypoints, descriptors = self.extractor.detectAndCompute(image, None)\n",
    "        return descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = {\n",
    "    \"SIFT\": SIFT,\n",
    "    \"SIFT_DENSE\": DenseSIFT,\n",
    "    \"KAZE\": KAZE,\n",
    "    \"AKAZE\": AKAZE,\n",
    "    \"ORB\": ORB,\n",
    "    \"BRISK\": BRISK\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images_filenames, train_labels, feature_extractor, extract_features=True):\n",
    "    descriptors = list()\n",
    "    label_per_descriptor = list()\n",
    "    images = list()\n",
    "\n",
    "    for filename, labels in zip(train_images_filenames, train_labels):\n",
    "        ima = cv2.imread(filename)\n",
    "        gray = cv2.cvtColor(ima, cv2.COLOR_BGR2GRAY)\n",
    "        if extract_features:\n",
    "            des = feature_extractor.extract_features(gray)\n",
    "            descriptors.append(des)\n",
    "        else:\n",
    "            images.append(gray)\n",
    "        label_per_descriptor.append(labels)\n",
    "    if not extract_features:\n",
    "        return images, label_per_descriptor\n",
    "    else:\n",
    "        return descriptors, label_per_descriptor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. bag of visual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_local_features(features, n_clusters):\n",
    "    codebook = MiniBatchKMeans(n_clusters=n_clusters, verbose=False, batch_size=n_clusters *\n",
    "                               20, compute_labels=False, reassignment_ratio=10**-4, random_state=42)\n",
    "    codebook.fit(features)\n",
    "    return codebook\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTOR = feature_extractors[\"SIFT\"]()\n",
    "train_descriptors, train_labels_descrip = extract_features(\n",
    "    train_images_filenames, train_labels, DESCRIPTOR)\n",
    "test_descriptors, test_labels_descrip = extract_features(\n",
    "    test_images_filenames, test_labels, DESCRIPTOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = np.vstack(train_descriptors)\n",
    "codebook = cluster_local_features(stack, n_clusters=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(assigned_clusters, num_clusters):\n",
    "    bag_visual_words = np.zeros(\n",
    "        (len(assigned_clusters), num_clusters), dtype=np.float32)\n",
    "    for i in range(len(assigned_clusters)):\n",
    "        hist_i, _ = np.histogram(\n",
    "            assigned_clusters[i], bins=num_clusters, range=(0, num_clusters))\n",
    "        bag_visual_words[i, :] = normalize(hist_i.reshape(1, -1), norm='l2')\n",
    "    return bag_visual_words\n",
    "\n",
    "\n",
    "def obtain_histogram_visual_words(features, tr_lengths=None, codebook=None):\n",
    "    if tr_lengths is None:\n",
    "        tr_lengths = [len(feature) for feature in features]\n",
    "        features = np.vstack(features)\n",
    "    assigned_labels = codebook.predict(features)\n",
    "    lengths = np.array(\n",
    "        [0]+[descriptor_length for descriptor_length in tr_lengths])\n",
    "    lengths = np.cumsum(lengths)\n",
    "    splitted_labels = [assigned_labels[lengths[i]:lengths[i+1]]\n",
    "                       for i in range(len(lengths)-1)]\n",
    "    return compute_histogram(splitted_labels, codebook.cluster_centers_.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words_train = obtain_histogram_visual_words(\n",
    "    train_descriptors, codebook=codebook)\n",
    "visual_words_test = obtain_histogram_visual_words(\n",
    "    test_descriptors, codebook=codebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, metric='euclidean')\n",
    "knn.fit(visual_words_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation functions\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "cv_strategies = {\n",
    "    \"kfold\": KFold,\n",
    "    \"stratified\": StratifiedKFold,\n",
    "    \"repeats\": RepeatedStratifiedKFold\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    \"balanced_accuracy\": balanced_accuracy_score\n",
    "}\n",
    "\n",
    "\n",
    "class BoVWClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Image classifier using Bag of Visual Words. \"\"\"\n",
    "\n",
    "    def __init__(self, clustering_method, classifier, reduction_method):\n",
    "        self.clustering_method = clustering_method\n",
    "        self.classifier = classifier\n",
    "        self.reduction_method = reduction_method\n",
    "        self.codebook = None\n",
    "\n",
    "    def fit(self, features, labels, sample_weight=None):\n",
    "        tr_lengths = [len(feature) for feature in features]\n",
    "        features = np.vstack(features)\n",
    "        self.codebook = self.clustering_method(features)\n",
    "        tr_hist = obtain_histogram_visual_words(\n",
    "            features, tr_lengths, self.codebook)\n",
    "        tr_hist_reduced = self.reduction_method.fit_transform(tr_hist, labels)\n",
    "        self.classifier.fit(tr_hist_reduced, labels)\n",
    "\n",
    "    def fit_transform(self, features, labels):\n",
    "        self.fit(features, labels)\n",
    "        return self.predict(features)\n",
    "\n",
    "    def predict_proba(self, features):\n",
    "        te_lengths = [len(feature) for feature in features]\n",
    "        features = np.vstack(features)\n",
    "\n",
    "        te_hist = obtain_histogram_visual_words(\n",
    "            features, te_lengths, self.codebook)\n",
    "        te_hist_reduced = self.reduction_method.transform(te_hist)\n",
    "        cls = self.classifier.predict_proba(te_hist_reduced)\n",
    "        return cls\n",
    "\n",
    "    def predict(self, features):\n",
    "        te_lengths = [len(feature) for feature in features]\n",
    "        features = np.vstack(features)\n",
    "\n",
    "        te_hist = obtain_histogram_visual_words(\n",
    "            features, te_lengths, self.codebook)\n",
    "        te_hist_reduced = self.reduction_method.transform(te_hist)\n",
    "        cls = self.classifier.predict(te_hist_reduced)\n",
    "        return cls\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return (sum(self.predict(X)))\n",
    "\n",
    "    def score_accuracy(self, X, y):\n",
    "        return 100*self.score(X, y)/len(y)\n",
    "\n",
    "\n",
    "class FastCrossValidator:\n",
    "    \"\"\" Cross-validator class \"\"\"\n",
    "\n",
    "    def __init__(self, cv_method, metric_name, trainer, labels):\n",
    "        \"\"\" \n",
    "        Params:\n",
    "        - cv_method (function): Clustering function that when called returns a codebook.\n",
    "        - classifier (Classifier like KNN, LogisticRegression,...)\n",
    "        - reduction_method (None/PCA/LDA/Isomap)\n",
    "        \"\"\"\n",
    "        self.cv_method = cv_method\n",
    "        self.metric_name = metric_name\n",
    "        self.trainer = trainer\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def cross_validate(self, feature_list, labels, n_jobs=-1):\n",
    "        return cross_val_score(self.trainer, feature_list, labels, scoring=self.metric_name, cv=self.cv_method, n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "class Dummy():\n",
    "    \"\"\" Dummy dimensionality reduction method that keeps all the original features. \"\"\"\n",
    "\n",
    "    def fit_transform(self, features, labels):\n",
    "        return features\n",
    "\n",
    "    def transform(self, features):\n",
    "        return features\n",
    "\n",
    "\n",
    "classifiers = {\"KNN\": KNeighborsClassifier}\n",
    "\n",
    "dim_reduction = {\n",
    "    \"None\": Dummy,\n",
    "    \"PCA\": PCA,\n",
    "    \"LDA\": LinearDiscriminantAnalysis,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words_test = obtain_histogram_visual_words(\n",
    "    test_descriptors, codebook=codebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 100*knn.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Dimensonality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "VWpca = pca.fit_transform(visual_words_train)\n",
    "knnpca = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, metric='euclidean')\n",
    "knnpca.fit(VWpca, train_labels)\n",
    "vwtestpca = pca.transform(visual_words_test)\n",
    "accuracy = 100*knnpca.score(vwtestpca, test_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=7)\n",
    "VWlda = lda.fit_transform(visual_words_train, train_labels)\n",
    "knnlda = KNeighborsClassifier(n_neighbors=5, n_jobs=-1, metric='euclidean')\n",
    "knnlda.fit(VWlda, train_labels)\n",
    "vwtestlda = lda.transform(visual_words_test)\n",
    "accuracy = 100*knnlda.score(vwtestlda, test_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 \n",
    "● Test different amounts of local features. What performs best?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_local_features(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "    n_features = int(trial.suggest_int('n_features', 50, 1000))\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT\"](n_features=n_features)\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=128)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=5, n_jobs=8, metric='euclidean')\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"SIFT\", direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_local_features, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"sift.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2\n",
    "Use dense SIFT instead of detected keypoints. Conclusions?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_models(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "    step_size = int(trial.suggest_int('step_size', 2, 100))\n",
    "    patch_size = int(trial.suggest_int('patch_size', 1, 5))\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](\n",
    "        step_size=step_size, patch_size=patch_size)\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=128)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=5, n_jobs=8, metric='euclidean')\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"DenseSIFT\",\n",
    "                            direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_models, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"densesift.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2\n",
    "● Test different amounts of codebook sizes k. What performs best?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "def compare_n_clusters(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](\n",
    "        step_size=75, patch_size=3)\n",
    "\n",
    "    n_clusters = int(trial.suggest_categorical(\n",
    "        'n_clusters', [64, 128, 256, 512, 1024]))\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=n_clusters)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=5, n_jobs=8, metric='euclidean')\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"DenseSIFT\",\n",
    "                            direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_n_clusters, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"n_clusters.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3\n",
    "● Test different values of k for the k-nn classifier. What performs best?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "def compare_k_classifier(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](\n",
    "        step_size=75, patch_size=3)\n",
    "\n",
    "    n_neighbors = int(trial.init_suggest_int('n_neighbors', 1, 10))\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=256)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=n_neighbors, n_jobs=8, metric='euclidean')\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"K classifier\", direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_k_classifier, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"K_classifier.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4\n",
    "● Test other distances in k-nn classifier. Does that make a difference? Why?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "def compare_distances_classifier(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "\n",
    "    n_neighbors = int(trial.init_suggest_int('n_neighbors', 1, 10))\n",
    "    metric = trial.suggest_categorical(\n",
    "        'metric', [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"])\n",
    "\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](\n",
    "        step_size=75, patch_size=3)\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=256)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=n_neighbors, n_jobs=8, metric=metric)\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"Distances\",\n",
    "                            direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_distances_classifier, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"distances.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "def compare_Mix_classifier(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "    metric = trial.suggest_categorical(\n",
    "        'metric', [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"])\n",
    "\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](\n",
    "        step_size=75, patch_size=3)\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=256)\n",
    "    dim_reduction_type = dim_reduction[\"None\"]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=5, n_jobs=8, metric=metric)\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"Mix knn\", direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_Mix_classifier, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"MIX_KNN.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5\n",
    "● Play with reducing dimensionality. Conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO\n",
    "def compare_dimensionality(trial):\n",
    "    search_metric = \"balanced_accuracy\"\n",
    "    cv_strategy = cv_strategies[\"stratified\"](n_splits=10)\n",
    "    DESCRIPTOR = feature_extractors[\"SIFT_DENSE\"](step_size=75, patch_size=3)\n",
    "\n",
    "    train_descriptors, train_labels_descrip = extract_features(\n",
    "        train_images_filenames, train_labels, DESCRIPTOR)\n",
    "    test_descriptors, test_labels_descrip = extract_features(\n",
    "        test_images_filenames, test_labels, DESCRIPTOR)\n",
    "\n",
    "    dimensionality_reduction = trial.suggest_categorical(\n",
    "        'dimensionality_reduction', [\"PCA\", \"LDA\"])\n",
    "\n",
    "    clustering = partial(cluster_local_features, n_clusters=256)\n",
    "    dim_reduction_type = dim_reduction[dimensionality_reduction]()\n",
    "    classifier = classifiers[\"KNN\"](\n",
    "        n_neighbors=5, n_jobs=8, metric='euclidean')\n",
    "\n",
    "    ex_trainer = BoVWClassifier(clustering, classifier, dim_reduction_type)\n",
    "    ex_cv = FastCrossValidator(\n",
    "        cv_strategy, search_metric, ex_trainer, np.unique(train_labels_descrip))\n",
    "    ex_metrics = ex_cv.cross_validate(\n",
    "        train_descriptors, train_labels_descrip, n_jobs=8)\n",
    "\n",
    "    return ex_metrics.mean()\n",
    "\n",
    "\n",
    "# random, grid search all of you want sampler https://optuna.readthedocs.io/en/stable/reference/samplers/index.html\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"Dimensionality Reduction\", direction='maximize', sampler=sampler)\n",
    "study.optimize(compare_dimensionality, n_trials=100, n_jobs=8)\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(\"compare_dimensionality.csv\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BEST MODEL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d50c3de034858aa5bc28c8d16217bb1fee78e4fccfb8e256f6ff5f2e44fbe88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
